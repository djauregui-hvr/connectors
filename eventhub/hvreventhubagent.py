#!python

################################################################################
#
#     Copyright (c) 2000-2019 HVR Software bv
#
################################################################################
#
# NAME
#     hvreventhubagent.py - Event Hub agent 
#
# SYNOPSIS
#     hvreventhubagent.py <mode> <chn> <loc> [<userargs>]
#
# DESCRIPTION
#     This script should be defined in an HVR channel using action AgentPlugin.
#     Its behaviour depends on the agent mode, environment variables, and
#     options supplied in parameter /UserArgument (see OPTIONS below).
#
# REQUIRED RUNTIME VALUES
#
#   The required information to connect to, and send data to, an Event Hub is:
#
#   -e name         Event Hub name.  The name of the Event Hub.  This information
#                   can also be provided by HVR_EVENTHUB_NAME.  The name of the 
#                   Event Hub can also be derived by the agent.  See 
#                   name_format and integ_fexpr below.
#
#   -k key          Event Hub Access Policy Key.  Can also be provided by
#                   HVR_EVENTHUB_KEY
#
#   -s namespace    Event Hub namespace.  The Event Hub is located in this
#                   namespace.  This information can also be provided by
#                   HVR_EVENTHUB_NAMESPACE
#
#   -u name         Event Hub Access Policy name.  Can also be provided by
#                   HVR_EVENTHUB_USER
#
# OPTIONS
#
#   -a address      The connection address.  Usually this is generated by 
#                   hvreventhubagent using the Event Hub nsmaespace and 
#                   name.  Use this option to explicitly set the address.
#
#   -b prefix       The before prefix string prepended to the before image column
#                   names. The default is 'Old&'.
#
#   -c c/a<d>       Collapse update before and after image messages into one after
#                   image record but adding the before columns with the column name
#                   prefixed by the prefix specified in -b. If '-cc' is specified, only the before
#                   values of the columns that actually changed are added to the message.
#                   If '-ca' is set, then all the befores are included.
#                   This option only affects hvp_op 4 and 2, all other pass through unchanged.
#                   If a 'd' is appended ('-cad' or '-ccd'), then if a row is unchanged,
#                   it will be discarded.
#                   **** THIS ASSUMES PAYLOAD IS JSON ROW_ARRY format ****
#
#   -d colname      If a column name is specified, the script will identify the last row in a
#                   transaciton and set the named column to '1'.  If this is used with -cad or
#                   -cdd then the script will not discard the last change so it can be marked.
#                   **** THIS ASSUMES PAYLOAD IS JSON ROW_ARRY format ****
#
#   -f fail/ok      If the integrate file does not exist, controls whether
#                   the agent fails or continues.  The default is to fail.  If set 
#                   to "-f ok" then a message will be logged and the agent
#                   will continue processing.
#
#   -h name_format  Alternative method of specifying the Event Hub name.
#                   An expression combining optional fixed fields with
#                   HVR variables that can be derived from the environment.
#                   Can also be specified using HVR_EVENTHUB_NAME_FORMAT
#  
#   -i integ_fexpr  Integrate file rename expression. Must be specified if the
#                   file name is the source for a variable used in name_format.
#                   Can also be specified using HVR_EVENTHUB_FILE_EXPR
#
#   -m msg_max      Any message that is greater than this value will be
#                   logged and then skipped.  Default is 1046500.
#
#   -o op_type_col  Column name that contain the hvr_op. Only has meaning for
#                   and is required when collapsing before and afters with option -c
#
#   -p partition_id Event Hub partition id.  If not specified the Event Hub 
#                   server will assign the partition id.
#                   Can also be specified using HVR_EVENTHUB_PARTITION
#
#   -r sent/end     Controls when the processed integrate files are removed.
#                   By default they are removed after the message has been sent.
#                   If set to "-r end", all the integrate files are removed
#                   at the end after all have been successfully sent.
#
#   -t trace        Enable tracing.  The following values are supported:
#                   0: disable tracing
#                   1: enable tracing
#                   2: enable tracing with more detail
#                   3: enable tracing include contents of every
#                      message sent to the Event Hub
#                   Can also be specified using HVR_EVENTHUB_TRACE
#
#   -x fail/ok      If the size of the integrate file exceeds the maximum
#                   message size (-m), controls whether the agent fails or
#                   continues.  The default is to fail.  If set to
#                   "-x ok" then a message will be logged and the agent
#                   will continue processing.
#
#
# AGENT OPERATION
#
#     The following modes are supported by this agent. Other modes are ignored.
#     All modes are meant to be run by HVR integrate/refresh via AgentPlugin
#     action.
#
#     integ_end
#         Send the contents of each integrate file to the Event Hub as the data
#         of an Event Hub message.
#
#     refr_write_end
#         Send the contents of each refresh file to the Event Hub as the data
#         of an Event Hub message.
#
# CHANGE_LOG
#     01/28/2021 RLR:  Allow both python2 and python3
#                      Script only supports azure-eventhub Python library 5.0.0 and above
#     02/08/2021  SW:  Added logic to collapse the before and after rows into one update row
#     02/09/2021 RLR:  Fixed script to run without collapse ligc by default
#                      Added a size check to the collapsed data
#     04/01/2021 RLR:  Import json to support the HVR_LONG_ENVIRONMENT logic
#     04/08/2021 RLR:  Added try/except block around JSON load
#     04/09/2021 RLR:  Changed check for op_type column
#     04/13/2021 RLR:  Added logic compute the partition id by modulo of transaction id
#                      Only works when the source is Oracle
#                      Configure:  HVR_EVENTHUB_PARTITION=TXNID
#                                  HVR_EVENTHUB_FILE_EXPR=<Integrate /RenameExpression>
#                                  Integrate /RenameExpression includes {hvr_tx_seq}
#     04/16/2021 RLR:  Fixed bug reading data file with UTF encoding
#     04/16/2021 RLR:  Remove all files in a batch after the batch is sent
#     04/23/2021 RLR:  Support hvr_integ_seq for the partition id assignment
#     04/26/2021 RLR:  Add batch logging
#                          Configure:  HVR_EVENTHUB_JOURNAL_BATCHES=<path to store journal files>
#                          Filename is <channel>_<loc>_<date>_<time>_<partitionid>_<seq>
#                      Modify collapse logic to ignore changes to specified columns
#                          Configure:  HVR_EVENTHUB_IGNORE_COLS=<comma separated list of column names>
#     04/28/2021 RLR:  Fixed the logic raising an exception if HVR_EVENTHUB_JOURNAL_BATCHES is invalid
#     04/29/2021 RLR:  Added the encoding option to the open of the journal file
#     05/18/2021 RLR:  If setting partition id based on transaction id in the file name,
#                      mark the end of every transaction in eventData.properties
#     05/20/2021 RLR:  Add option to skip unchanged rows in collapse logic
#     05/21/2021 RLR:  Fixed a syntax bug
#     05/26/2021 RLR:  Fixed logic to skip unchanged rows in collapse logic (was skipping inserts)
#     05/27/2021 RLR:  Added logic to mark the end of the transaction by setting the column       
#                      value of a user column to '1' for the last row in a transaction
#     06/16/2021 RLR:  Set value of end transaction marker to 1 instead of '1'
#     07/23/2021 RLR:  Set encoding on all file open calls
#     07/28/2021 RLR:  If the open of a file fails, skip that file and continue processing.
#     07/29/2021 RLR:  Remove the trace lines dumping file content - caused encoding error
#     08/03/2021 RLR:  Make sure that only the last change gets the end-transaciton marker
#     08/12/2021 RLR:  Make the 'processed file' name unique to the channel/location
#
################################################################################


import sys
import getopt
import os
import re
import logging
import traceback
import json
import platform
from timeit import default_timer as timer
from azure.eventhub import EventHubProducerClient, EventData
from azure.eventhub.exceptions import EventHubError

logger = logging.getLogger("azure")

class Options:
    me = ''
    mode = ''
    channel = ''
    location = ''
    ehub_namespace = ''
    ehub_address = ''
    ehub_user = ''
    ehub_key = ''
    ehub_partition = ''
    ehub_name = ''
    ehub_name_pattern = []
    file_pattern = []
    file_expression = ''
    name_format = ''
    partition_ids = []
    filenames = []
    file_path = ''
    processed_file = ''
    maximum_message_size = 1046500
    process_json_files = False
    collapse_befores = ''
    op_type_col = ''
    before_prefix = '&Old'
    ignore_columns = []
    end_trans_col = ''
    debug_mode = False
    remove_each_file_as_done = True
    fail_if_file_not_exist = True
    fail_if_file_too_large = True
    trace = 0
    jnl_batches = ''
    jnl_fd = None
    fail_files_loc = ''

file_counter   = 0
byte_total     = 0
bytes_in_files = 0
options = Options()

##### Support function #########################################################
class AgentError(Exception):
    pass

def version_normalizer(version):
    version_array = version.split('.')
    version_num = 0
    least_significant = len(version_array) - 1
    if least_significant > 2:
        least_significant = 2
    shift = 1
    for i in range(least_significant, -1, -1):
        version_num += int(version_array[i]) * shift
        shift *= 1000
    return version_num
    
def version_check():
    global python3
    python3 = sys.version_info[0] == 3

def print_raw(_msg, tgt= None):
    _msg= re.sub(r'!\{[^}]*\}!', '!{xxxxxxxx}!', _msg)

    if tgt is None:
        tgt= sys.stdout

    tgt.write(_msg)
    tgt.flush()

def print_message(_msg, *args, **kwargs):
    _msg= str(_msg).format(*args, **kwargs)

    # convert newlines to <space><backslash><newline><space> like HVR exceptions
    _msg= _msg.replace("\n", " \\\n ")
    print_raw("{0}\n".format(_msg))

def trace(level, _msg, *args, **kwargs):
    global options
    if level > options.trace:
         return
    header = "TRACE=" + str(level) + ": "
    _msg= str(_msg).format(*args, **kwargs)
    _msg= _msg.replace("\n", " \\\n ")
    _msg= header + _msg
    print_raw("{0}\n".format(_msg))

def load_agent_env():
    agent_env= {}

    if 'HVR_LONG_ENVIRONMENT' in os.environ:
        hvr_long_environment= os.environ['HVR_LONG_ENVIRONMENT']
        try:
            with open(hvr_long_environment, "r", encoding="utf8") as f:
                long_env= json.loads(f.read())

            for k,v in long_env.items():
                agent_env[str(k)]= str(v)

        except Exception as e:
            sys.stderr.write( ("W_JX0E00: Warning: An error occured while "
                               "processing $HVR_LONG_ENVIRONMENT file "
                               "'{}'. Will continue without processing this "
                               "file. Error: {} {}").format(
                                   hvr_long_environment,
                                   str(e),
                                   traceback.format_exc()) )

    for k,v in os.environ.items():
        k= str(k)
        if k not in agent_env:
            agent_env[k]= str(v)

    return agent_env

def run_with_ev_abbr(evname, evval):
    global ev_warn

    if ev_warn is None and os.getenv('HVR_EVENTHUB_AGENT_WARN_ABBR'):
        ev_warn = os.getenv('HVR_EVENTHUB_AGENT_WARN_ABBR').split(':')

    # Env var is abbreviated due to long size
    if ev_warn is not None and evname in ev_warn:
        sys.stderr.write( ("W_JX0E00: Warning: environment "
                           "variable ${} is truncated at {} "
                           "characters possibly due to long size. "
                           "This is ignored because of "
                           "$HVR_EVENTHUB_AGENT_WARN_ABBR."
                          ).format(evname, len(evval)) )
        return True
    raise AgentError( ("Environment variable ${} is truncated "
                       "at {} characters possibly due to long "
                       "size. This environment variable is "
                       "important for hvreventhub to "
                       "function as intended. Refusing to "
                       "continue. Truncated value: {{ {} }}"
                      ).format(evname, len(evval), evval) )

def load_environment():
    evars = load_agent_env()

    if 'HVR_EVENTHUB_NAMESPACE' in evars:
        options.ehub_namespace = evars['HVR_EVENTHUB_NAMESPACE']
    if 'HVR_EVENTHUB_ADDRESS' in evars:
        options.ehub_address = evars['HVR_EVENTHUB_ADDRESS']
    if 'HVR_EVENTHUB_USER' in evars:
        options.ehub_user = evars['HVR_EVENTHUB_USER']
    if 'HVR_EVENTHUB_KEY' in evars:
        options.ehub_key = evars['HVR_EVENTHUB_KEY']
    if 'HVR_EVENTHUB_NAME' in evars:
        options.ehub_name = evars['HVR_EVENTHUB_NAME']
    if 'HVR_EVENTHUB_PARTITION' in evars:
        options.ehub_partition = evars['HVR_EVENTHUB_PARTITION']
        if options.ehub_partition.upper() == 'TXNID':
            options.ehub_partition = 'TXNID'
    if 'HVR_EVENTHUB_FILE_EXPR' in evars:
        options.file_expression = evars['HVR_EVENTHUB_FILE_EXPR']
    if 'HVR_EVENTHUB_NAME_FORMAT' in evars:
        options.name_format = evars['HVR_EVENTHUB_NAME_FORMAT']
    if 'HVR_EVENTHUB_TRACE' in evars:
        options.trace = int(evars['HVR_EVENTHUB_TRACE'])
    if 'HVR_EVENTHUB_JOURNAL_BATCHES' in evars:
        options.jnl_batches = evars['HVR_EVENTHUB_JOURNAL_BATCHES']
        if not os.path.exists(options.jnl_batches):
            pi = platform.uname()
            raise Exception("Path given for 'HVR_EVENTHUB_JOURNAL_BATCHES', {}, does not exist on {}".format(options.jnl_batches, pi.node))
    if 'HVR_INVALID_FILE_LOC' in evars:
        options.fail_files_loc = evars['HVR_INVALID_FILE_LOC']
        if not os.path.exists(options.fail_files_loc):
            pi = platform.uname()
            raise Exception("Path given for 'HVR_INVALID_FILE_LOC', {}, does not exist on {}".format(options.fail_files_loc, pi.node))
    if 'HVR_EVENTHUB_IGNORE_COLS' in evars:
        options.ignore_columns = evars['HVR_EVENTHUB_IGNORE_COLS'].split(',')
    if 'HVR_FILE_LOC' in evars:
        options.file_path = evars['HVR_FILE_LOC']
    if 'HVR_FILE_NAMES' in evars and evars['HVR_FILE_NAMES']:
        options.filenames = evars['HVR_FILE_NAMES'].split(':')
        if options.filenames[-1].endswith("..."):
            if run_with_ev_abbr('HVR_FILE_NAMES', evars['HVR_FILE_NAMES']):
                del options.filenames[-1]

def print_environment():
    if options.trace < 2:
        return
    trace(2, "============================================")
    env = os.environ
    if python3:
        for key, value  in env.items():
            if key.find('HVR') != -1:
                trace(2, "{} = {}", key, value)
    else:
        for key, value  in env.iteritems():
            if key.find('HVR') != -1:
                trace(2, "{} = {}", key, value)
    trace(2, "============================================")

def print_options():
    if options.trace < 1:
        return
    if not options.filenames and options.trace < 2:
        return
    trace(1, "============================================")
    trace(1, "mode = {}", options.mode)
    trace(1, "channel = {}", options.channel)
    trace(1, "location = {}", options.location)
    trace(1, "EventHub namespace = {}", options.ehub_namespace)
    trace(1, "EventHub address = {}", options.ehub_address)
    trace(1, "EventHub user = {}", options.ehub_user)
    trace(1, "EventHub key = {}", options.ehub_key)
    trace(1, "EventHub partition = {}", options.ehub_partition)
    trace(1, "EventHub name = {}", options.ehub_name)
    trace(1, "Filename list = {}", options.filenames)
    trace(1, "Filename location = {}", options.file_path)
    trace(1, "Integrate filename expression = {}", options.file_expression)
    trace(1, "Integrate filename elements = {}", options.ehub_name_pattern)
    trace(1, "EventHub name format = {}", options.name_format)
    trace(1, "EventHub name elements = {}", options.file_pattern)
    trace(1, "Maximum message size = {}", options.maximum_message_size)
    if options.remove_each_file_as_done:
        removal_policy = "'sent': as each file is sent"
    else:
        removal_policy = "'end': after all files are sent"
    trace(2, "Remove files = {}", removal_policy)
    if options.fail_if_file_not_exist:
        arg_action = "'fail': fail the script"
    else:
        arg_action = "'ok': skip the file and continue processing"
    trace(2, "If the integrate file does not exist = {}", arg_action)
    if options.fail_if_file_too_large:
        arg_action = "'fail': fail the script"
    else:
        arg_action = "'ok': skip the file and continue processing"
    trace(2, "If the integrate file size exceeds the maximum message size = {}", arg_action)
    if options.ehub_partition:
        if options.ehub_partition == 'TXNID':
            trace(2, "Assign events to partition based on transaction ID")
        else:
            trace(2, "Assign events to partition {}".format(options.ehub_partition))
    trace(2, "Data processing if /Json /ROW_ARRAY = {}".format(options.process_json_files))
    if options.process_json_files:
        trace(2, "  Column to mark end-of-transaction = {}".format(options.end_trans_col))
        trace(2, "  Collapse before and after images into one after = {}", options.collapse_befores)
        if options.collapse_befores:
            trace(2, "    Column name containing {{hvr_op}} = {}", options.op_type_col)
            trace(2, "    Ignore changes to {}", options.ignore_columns)
            trace(2, "    Before column prefix = {}", options.before_prefix)
    trace(2, "debug_mode = {}", options.debug_mode)
    trace(2, "Journal batches sent = {}", options.jnl_batches)
    trace(3, "Processed files = {}".format(options.processed_file))
    trace(1, "trace = {}", options.trace)
    trace(1, "============================================")

def usage(extra):
    """
        Debug options are:
           [-z on/off] - to turn on debug mode
    """
    if extra is not None:
        print_message('')
        print_message(extra)
    print_message('')
    print_message("Usage: {} <mode> <chn> <loc> [userargs]", options.me)
    print_message('')
    print_message("        where userargs= [-e Event Hub name]")
    print_message("                        [-k Event Hubs shared access key]")
    print_message("                        [-s Event Hubs Namepsace]")
    print_message("                        [-u Event Hubs shared access name]")
    print_message("                        [-a Event Hub address]")
    print_message("                        [-f fail/ok] (file not exist is ok or fail, default=fail)")
    print_message("                        [-h event hub name expression")
    print_message("                        [-i integrate rename expression")
    print_message("                        [-m Maximum message length] (default=1047500)")
    print_message("                        [-p partition id]")
    print_message("                        [-c [c|a] Collapse before and after images into one after image.")
    print_message("                                  [c] includes only changed before columns.")
    print_message("                                  [a] includes all before columns, not just those that changed.")
    print_message("                        [-o column name containing {{hvr_op}} column when using -c option")
    print_message("                        [-b before prefix (prefix for before col names when used with -c. Default is Old&")
    print_message("                        [-r sent/end] (remove files when sent or at end, default=sent)")
    print_message("                        [-t trace level]")
    print_message("                        [-x fail/ok] (file too large is ok or fail, default=fail)")
    print_message('')
    exit(1)

def get_options(argv):
    global options
    global ev_warn

    ev_warn = None
    options.me= os.path.basename(argv[0])
    if len(argv) < 4:
        usage('Missing mode,chn,loc arguments');
    elif len(argv) > 5:
        usage('Extra arguments after userargs argument')

    options.mode= argv[1]
    options.channel= argv[2]
    options.location= argv[3]
    userargs= []
    if len(argv) >= 5 and argv[4]:
        userargs= re.split(r'\s+', argv[4].strip())

    if options.mode not in ['integ_begin','integ_end',
            'refr_write_begin','refr_write_end']:
        # Don't bother parsing options. (integ_begin & refr_write_begin are
        # no-op, are included in list to raise agent errors early)
        usage('Missing valid mode: one of [integ_begin,integ_end,refr_write_begin,refr_write_end]')

    load_environment()

    if userargs:
        try:
#           opts, args= getopt.getopt(userargs, 's:a:u:k:e:m:p:i:h:r:c:o:b:f:t:x:z:')
            opts, args= getopt.getopt(userargs, 'a:b:c:d:e:f:h:i:k:m:o:p:r:s:t:u:x:z:')
        except getopt.GetoptError as e:
            usage(str(e))
    
        for (opt_key, opt_val) in opts:
            if opt_key == '-a':  
                options.ehub_address = opt_val
            elif opt_key == '-b':
                options.before_prefix = opt_val
            elif opt_key == '-c':
                options.collapse_befores = opt_val
            elif opt_key == '-d':
                options.end_trans_col = opt_val
            elif opt_key == '-e': 
                options.ehub_name = opt_val
            elif opt_key == '-f':
                options.fail_if_file_not_exist = (opt_val != 'ok')
            elif opt_key == '-h':
                options.name_format = opt_val
            elif opt_key == '-i':
                options.file_expression = opt_val
            elif opt_key == '-k':
                options.ehub_key = opt_val
            elif opt_key == '-m':
                options.maximum_message_size = int(opt_val)
            elif opt_key == '-o':
                options.op_type_col = opt_val
            elif opt_key == '-p':
                options.ehub_partition = opt_val
            elif opt_key == '-r':
                options.remove_each_file_as_done = (opt_val != 'end')
            elif opt_key == '-s':
                options.ehub_namespace = opt_val
            elif opt_key == '-t':
                options.trace= int(opt_val)
            elif opt_key == '-u':
                options.ehub_user = opt_val
            elif opt_key == '-x':
                options.fail_if_file_too_large = (opt_val != 'ok')
            elif opt_key == '-z':
                options.debug_mode = (opt_val == 'on')

    define_processed_file()

    if options.collapse_befores or options.end_trans_col:
        options.process_json_files = True

    trace(2, "Arguments {} {} {} {}", options.mode, options.channel, options.location, userargs)
    if userargs and args:
        usage('extra userargs specified')

def validate_options():
    if options.ehub_namespace:
        if options.ehub_address:
            trace(2, "Configured namespace {} ignored since address is provided: {}", options.ehub_namespace, options.ehub_address)
        else:
            if not options.ehub_user or not options.ehub_key:
                usage("Authentication for the Event Hub is not defined - define key name ([-u name]/HVR_EVENTHUB_USER) and value ([-k key]/HVR_EVENTHUB_KEY)")
            options.ehub_address = "sb://" + options.ehub_namespace + ".servicebus.windows.net/"
            options.ehub_address += ";SharedAccessKeyName=" + options.ehub_user 
            options.ehub_address += ";SharedAccessKey=" + options.ehub_key 
#    options.ehub_address="Endpoint=sb://rlrevents.servicebus.windows.net/;SharedAccessKeyName=robin;SharedAccessKey=vgq+a4mOstzEa+0pafeYH7WNjwTxlaL45mx2dZksZRE="

    if not options.ehub_address:
        usage("The Event Hub namespace is not defined - define using [-s namespace] or HVR_EVENTHUB_NAMESPACE")

    if options.file_expression and options.name_format:
        options.file_pattern = parse_expression(options.file_expression)
        options.ehub_name_pattern = parse_expression(options.name_format)
        if not have_ehub_name_elements():
            usage("If an event hub name format is defined ([-h name format] or HVR_EVENTHUB_NAME_FORMAT); the variable elements must be in the integrate rename expression as provided by [-i integrate expression] or HVR_EVENTHUB_FILE_EXPR")
    elif options.name_format:
        usage("If an event hub name format is defined ([-h name format] or HVR_EVENTHUB_NAME_FORMAT); the integrate rename expression must also be defined via [-i integrate expression] or HVR_EVENTHUB_FILE_EXPR")

    if options.ehub_partition == 'TXNID':
        if not options.file_expression:
            raise Exception("If HVR_EVENTHUB_PARTITION='TXNID', HVR_EVENTHUB_FILE_EXPR must also be set to Integrate /RenameExpression")
        if not options.file_pattern:
            options.file_pattern = parse_expression(options.file_expression)
        if not have_partition_id():
            raise Exception("The Integrate rename expression must include {hvr_tx_seq} or {hvr_integ_seq} to support partition assignment")
        
    if not options.ehub_name and not options.ehub_name_pattern:
        usage("The Event Hub name is not defined - define using [-e name] or HVR_EVENTHUB_NAME")

def parse_expression(filename_expression):
    elems = []
    for part in re.split(r'({[^}]*})', filename_expression):
        if not part:
            continue
        elems.append(str(part))
    trace(2, "parse {}", filename_expression)
    trace(2, "   result {}", elems)
    return elems

def have_ehub_name_elements():
    for part in options.ehub_name_pattern:
        if part.startswith("{"):
            if not part in options.file_pattern:
                if part != '{hvr_chn_name}' and part != '{hvr_integ_loc}':
                    trace(1, "{} not found in {}", part, options.file_expression)
                    return False
    return True

def have_partition_id():
    for part in options.file_pattern:
        if part == '{hvr_tx_seq}':
            return True
        if part == '{hvr_integ_seq}':
            return True
    return False

def parse_filename(filename):
    # break elements out of filename
    # return a dict
    trace(2, "parse {}", filename)
    vals = {}
    c1 = c2 = 0
    key = ""
    for part in options.file_pattern:
        if part.startswith("{"):
            key = part
            continue
        c2 = filename.find(part, c1)
        if c2 > c1:
            vals[key] = filename[c1:c2]
        c1 = c2 + len(part)
    if not key in vals.keys():
        vals[key] = filename[c1:]
    if options.trace > 1:
        show = ''
        for k,v in vals.items():
            show += k + ":" + v + "  "
        trace(2, "   result {}", show)
    return vals

def eventhub_name(file_values):
    name = ''
    for part in options.ehub_name_pattern:
        if not part.startswith("{"):
            name += part
        else:
            if part in file_values:
                name += file_values[part]
            elif part == "{hvr_chn_name}":
                name += options.channel
            elif part == "{hvr_integ_loc}":
                name += options.location
            else:
                print_message("Value for {} missing when building eventhub name", part[1:len(part)-1])
    trace(2, "eventhub_name: " + name)
    return name

def make_new_address(hvurl, eventhub):
    trace(1, "make_new_address from {} and {}", hvurl, eventhub)
    if python3:
        from urllib.parse import urlparse
    else:
        from urlparse import urlparse
    p = urlparse(hvurl)
    p = p._replace(path=eventhub)
    trace(1, "   new url {}", p.geturl())
    return p.geturl()


def process_row_data(array_data, end_transaction):
    trace(2, "process_row_data()")
    import json
    from json.decoder import JSONDecodeError
    expecting_after = False
    last_row = {}
    output_array = []
    update_before_op = 4
    update_after_op = 2
    all_before_columns = ('a' in options.collapse_befores)
    discard_no_change = ('d' in options.collapse_befores)

    if options.end_trans_col:
        trace(2, "process_row_data end-transaction = {}".format(end_transaction))

    try:
        json_array = json.loads(array_data)
    except JSONDecodeError as e:
        raise Exception("JSON decode error: {}; note that the 'collapse before/after' logic expects JSON/ROW_ARRAY".format(e))

    for row in json_array:
        if options.collapse_befores and options.mode == "integ_end":
            if options.op_type_col not in row.keys():
                raise Exception('/Extra column {} is required with /IntegrateExpression={{hvr_op}}'.format(options.op_type_col))
            else:
                op_type = row[options.op_type_col]
    
            if op_type == update_before_op:
                # Save the before image
                expecting_after = True
                before_row = row
            else:
                changed_cols = 0
                if expecting_after:
                    if op_type != update_after_op:
                        raise Exception('Expected after image immediately after before image. Received = {}'.format(op_type))
                    else:
                        # Process the after image
                        expecting_after = False
                        for key, before_value in before_row.items():
                            if key == options.op_type_col or key in options.ignore_columns:
                                continue
                            if row[key] != before_value:
                                changed_cols += 1
                            if all_before_columns:
                                row[options.before_prefix + key] = before_value
                            else:
                                if row[key] != before_value:
                                    row[options.before_prefix + key] = before_value
                    if changed_cols==0 and discard_no_change:
                        # to work around a failure in trace
                        msg = "Discard unchanged row: {}".format(row)
                        msg = msg.replace('{','(')
                        msg = msg.replace('}',')')
                        trace(2, msg)
                        last_row = row
                        continue
                last_row = {}
                output_array.append(row)
        else:
            output_array.append(row)
    if options.end_trans_col and end_transaction and last_row:
        output_array.append(last_row)
    if not output_array:
        return ''
    if options.end_trans_col and end_transaction and options.end_trans_col in output_array[-1]:
        output_array[-1][options.end_trans_col] = 1
    return json.dumps(output_array)

##### Main function ############################################################
def connect(ehub_name = None):
    global options

    if ehub_name is None:
        ehub_name = options.ehub_name
    trace(1, "Connect to {}", ehub_name)
    # print("connect using {}".format(options.ehub_address))
    try:
        producer = EventHubProducerClient.from_connection_string("Endpoint="+options.ehub_address, eventhub_name=ehub_name)
    except EventHubError as eh_err:
        print("Error connecting to {0} using {1}".format(options.ehub_address, ehub_name))
        raise eh_err
    
    if options.ehub_partition == 'TXNID':
        options.partition_ids = producer.get_partition_ids()
        if int(options.partition_ids[-1]) +1 != len(options.partition_ids):
            print("Partition list has {} ids, last is {}".format(len(options.partition_ids), options.partition_ids[-1]))
            raise Exception("Partition list is not consistent; cannot continue with partition-by-transactionid logic")

    return producer

def hub_filename_map():
    trace(1, "Build event hub name - filename map")
    hub_map = {}
    for fl in options.filenames:
        vals = parse_filename(fl)
        hub_name = eventhub_name(vals)
        if not hub_name in hub_map:
            hub_map[hub_name] = []
        hub_map[hub_name].append(fl)

    if options.trace:
        for hn in hub_map:
            trace(2, "Send to {}", hn)
            for fn in hub_map[hn]:
                trace(2, "  {}", fn)

    return hub_map

def save_invalid_file(file_name):
    if not options.fail_files_loc:
        return
    base = os.path.split(file_name)
    if not base[0]:
        trace(2, "Cannot rename {}; no base file name".format(file_name))
        return
    new_name = os.path.join(options.fail_files_loc, base[0])
    trace(2, "Rename failed file {} to {}".format(file_name, new_name))
    try:
        os.rename(file_name, new_name)
    except Exception as err:
        print("Error {} renaming {} to {}".format(err, file_name, new_name))

def contents_of_integ_file(file_name, end_trans):
    content = None
    try:
        with open(file_name, encoding="utf8") as file_fd:
            content = file_fd.read()
    except Exception as err:
        print("Error {} opening {}; skipping file".format(err, file_name))
        save_invalid_file(file_name)
        return ''
    trace(2, "File ({} bytes) {}", len(content), file_name)
    if options.process_json_files:
        # reformat the file content
        len_b4 = len(content)
        content = process_row_data(content, end_trans)
        if len(content) > options.maximum_message_size:
            raise AgentError( ("File {} ({} bytes), after update processing is {} bytes. Cannot send because of maximum message "
                           "limit {}. Adjust Integrate /MaxFileSize to something lower.".format(file_name, len_b4, len(content), options.maximum_message_size)))
        trace(2, "   {} bytes", len(content))

    return content
	
def define_processed_file():
    base_dir = os.getenv('HVR_LOC_STATEDIR', '/tmp')
    options.processed_file = os.path.join(base_dir, "{}_{}_{}.processed".format(options.me[:-3], options.channel, options.location))

def reset_processed_file():
    trace(3, "Delete {}".format(options.processed_file))
    try:
        if os.path.exists(options.processed_file):
            trace(2, "(reset) {}".format(options.processed_file))
            os.remove(options.processed_file)
    except Exception as err:
        trace(2, "{} removing 'processed' file {}", err, options.processed_file)
        pass
	
def read_processed_files():
    trace(3, "Read {}".format(options.processed_file))
    fnames = []
    try:
        if not os.path.exists(options.processed_file):
            return fnames
        with open(options.processed_file, 'r', encoding="utf8") as f:
            fnames = f.read().split(':')
    except Exception as err:
        trace(2, "{} reading 'processed' file {}", err, options.processed_file)
        pass
    trace(3, "Processed files {}".format(fnames))
    return fnames
	
def remove_uploaded_files(files):
    trace(3, "Add processed files {}".format(files))
    try:
        prepend = os.path.exists(options.processed_file)
        with open(options.processed_file, 'a', encoding="utf8") as f:
            if prepend:
                f.write(':')
            f.write(":".join(files))
            f.flush()
    except Exception as err:
        trace(2, "{} writing 'processed' file {}", err, options.processed_file)
        pass
    for fname in files:
        full_path = options.file_path + '/' + fname
        trace(2, "Remove {}".format(full_path))
        if os.path.exists(full_path):
            os.remove(full_path)

def journal_batch(producer, partition_id, message=None):
    if not options.jnl_batches:
        return
    if message and not options.jnl_fd:
        from time import gmtime, strftime
        tstamp = strftime("%Y%m%d_%H%M%S", gmtime())
        pid = -1
        seq = 999
        if partition_id:
            pid = partition_id
            partprops = producer.get_partition_properties(partition_id)
            seq = partprops['last_enqueued_sequence_number']
        jnlname = "{}_{}_{}_p{}_s{}".format(options.channel, options.location, tstamp, pid, seq)
        jnlpath = os.path.join(options.jnl_batches, jnlname)
        try:
            options.jnl_fd = open(jnlpath, "w", encoding="utf8")
        except Exception as ex:
            print("Cannot create journal file {}:{}".format(jnlpath, ex))
            return
    if not message and options.jnl_fd:
        options.jnl_fd.close()
        options.jnl_fd = None
        return
    options.jnl_fd.write(message)
    options.jnl_fd.write('\n')

def add_event_to_batch(event_data_batch, event):
    try:
        event_data_batch.add(event)
    except ValueError:
        return False
    except EventHubError as eh_err:
        print("Error adding data to batch")
        raise eh_err
    return True

def split_files_for_partitions(filelist):
    # The following works with a {hvr_tx_seq} value
    #   generated by an Oracle source
    num_partitions = len(options.partition_ids)
    split_list = []
    txid_list = []
    for i in range(0,num_partitions):
        split_list.append([])
        txid_list.append([])
    trace(2, "Split {} files into {} lists".format(len(filelist), num_partitions))
    for i, fname in enumerate(filelist):
        pid = 0
        file_values = parse_filename(fname)
        txseqstr = ''
        if '{hvr_tx_seq}' in file_values:
            txseqstr = file_values['{hvr_tx_seq}']
            if len(txseqstr) == 24:
                txseqstr = txseqstr[4:-4]
        if '{hvr_integ_seq}' in file_values:
            txseqstr = file_values['{hvr_integ_seq}']
            if len(txseqstr) == 36:
                txseqstr = txseqstr[4:-16]
        try:
            txseq = int(txseqstr, 16)
        except:
            txseq = 0
        pid = txseq % num_partitions
        trace(2, "Send {} to partition {} based on {}/{}", fname, pid, txseqstr, txseq)
        split_list[pid].append(fname)
        txid_list[pid].append(txseq)
    for i in range(0,num_partitions):
        trace(2, "  partition {}: {} files".format(i, len(split_list[i])))
    return split_list,txid_list

def reverse_item(L):
   for index in reversed(range(len(L))):
      yield index, L[index]

def get_end_trans_list(txid_list):
    txids = []
    eot = []

    for i in range(0, len(txid_list)):
        eot.append(0)
    for index, txid in reverse_item(txid_list):
        if not txid in txids:
            txids.append(txid)
            eot[index] = 1
    return eot

def send_batch(producer, start, files, eotlist=[], asked_partitionid = None):
    global bytes_in_files
    global byte_total

    if asked_partitionid:
        event_data_batch = producer.create_batch(max_size_in_bytes=options.maximum_message_size,partition_id=asked_partitionid)
    else:
        event_data_batch = producer.create_batch(max_size_in_bytes=options.maximum_message_size)
    newstart = len(files)
    for i in range(start, len(files)):
        if eotlist:
            end_trans = eotlist[i]
        else:
            end_trans = 0
        full_name = options.file_path + '/' + files[i]
        file_as_string = contents_of_integ_file(full_name, end_trans)
        if not file_as_string:
            continue
        event = EventData(file_as_string)
        if add_event_to_batch(event_data_batch, event):
            bytes_in_files += len(file_as_string)
            journal_batch(producer, asked_partitionid, file_as_string)
        else:
            trace(2, "Cannot add last file; batch full {}".format(event_data_batch.size_in_bytes))
            newstart = i
            break

    if not event_data_batch.size_in_bytes:
        trace(2, "Batch empty (maybe due to unchanged rows skipped); skip to next batch")
    else:
        if options.trace > 0:
            partinfo = ''
            if asked_partitionid:
                partinfo = ", partition_id = {}".format(asked_partitionid)
            trace(1, "Send batch {} bytes{}".format(event_data_batch.size_in_bytes, partinfo))
        try:
            producer.send_batch(event_data_batch)
        except EventHubError as eh_err:
            print("Error uploading ")
            raise eh_err
        journal_batch(producer, asked_partitionid, None)
        byte_total += event_data_batch.size_in_bytes

    return newstart   

def upload_set_of_files(producer, files, eotlist=[], partition_id=None):
    global file_counter

    if not files:
        trace(2, "No files to upload for partition {}", partition_id)
        return
    start_time = timer()
    bytes_before = byte_total
    start = 0
    while start < len(files):
        nextstart = send_batch(producer, start, files, eotlist, partition_id)
        file_counter += len(files[start:nextstart])
        if options.remove_each_file_as_done:
            remove_uploaded_files(files[start:nextstart])
        start = nextstart
    
    elapsed = timer() - start_time
    props = producer.get_eventhub_properties()
    target = props['eventhub_name']
    if partition_id:
        target += ", partition {}, ".format(partition_id)
    print("Upload of {} files, {} bytes, to {} took {:.2f} seconds".format(len(files), byte_total-bytes_before, target, elapsed))

def upload_files(producer, files):
    if options.partition_ids and options.mode == "integ_end":
        split_list,txid_list = split_files_for_partitions(files)
        for partid in options.partition_ids:
            end_trans_list = get_end_trans_list(txid_list[int(partid)])
            upload_set_of_files(producer, split_list[int(partid)], end_trans_list, partid)
    else:
        if options.ehub_partition and not options.partition_ids:
            upload_set_of_files(producer, files, [], options.ehub_partition)
        else:
            upload_set_of_files(producer, files)

def upload_to_eventhubs_identified_by_values():
    trace(1, "upload_to_eventhubs_identified_by_values")
    hub_map = hub_filename_map()
    for hub in hub_map:
        producer =  connect(hub)
        with producer:
            upload_files(producer, hub_map[hub])

def upload_to_configured_eventhub():
    trace(1, "upload_to_configured_eventhub: {}", options.ehub_name)
    producer = connect()
    with producer:
        upload_files(producer, options.filenames)

def file_exists(fname):
    full_name = options.file_path + '/' + fname
    if not os.path.exists(full_name):
        if not options.fail_if_file_not_exist:
            print_message("File {} does not exist", fname)
            return False
        # return true for later processing
        return True
    return True

def file_can_be_posted(fname):
    full_name = options.file_path + '/' + fname
    if options.maximum_message_size > 0 and os.stat(full_name).st_size > options.maximum_message_size:
        print_message("File {}, length {}, exceeds maximum {}; cannot send to EventHub", full_name, os.stat(full_name).st_size, options.maximum_message_size)
        return False
    return True

def verify_files_for_processing():
    global file_counter

    files_processed = read_processed_files()
    if files_processed:
        trace(2, "Skipping {} files as they have already been processed", len(files_processed))
        for fname in files_processed:
            if fname in options.filenames:
                file_counter += 1
                print_message("File {} already processed - will skip", fname)
                options.filenames.remove(fname)
    # check for files that do not exist and remove from the filelist
    badlist = [fname for fname in options.filenames if not file_exists(fname)]
    if badlist:
        for fname in badlist:
            options.filenames.remove(fname)
    # check for files that exceed the maximum message size and either fail or remove from the list
    badlist = [fname for fname in options.filenames if not file_can_be_posted(fname)]
    if badlist and options.fail_if_file_too_large:
        raise AgentError( ("Cannot send {} file(s) because of maximum message "
                           "limit {}. Either remove these files or set argument"
                           " [-x ok] to skip and continue processing. Affected"
                           " file(s): {}".format(len(badlist), 
                                                 options.maximum_message_size,
                                                 str(badlist)) ) )
    if badlist:
        for fname in badlist:
            options.filenames.remove(fname)


def cleanup_when_done():
    if options.debug_mode:
        return
    remove_uploaded_files(option.filenames)

def file_loc_processing():
    global file_counter
    global byte_total
    global bytes_in_files

    file_counter = byte_total = bytes_in_files = 0
    verify_files_for_processing()
    if not options.filenames:
        return
    if options.ehub_name_pattern and options.file_pattern:
        upload_to_eventhubs_identified_by_values()
    else:
        upload_to_configured_eventhub()
    if not options.remove_each_file_as_done:
        cleanup_when_done()
    
def report(eventhub_name = None):
    global file_counter
    global byte_total

    if not file_counter:
        return
    msg = str(file_counter) + " files"
    if eventhub_name is not None and eventhub_name:
        msg += " to " + eventhub_name
    if bytes_in_files:
        msg += "; " + str(bytes_in_files) + " bytes read"
    if byte_total:
        msg += "; " + str(byte_total) + " bytes sent"
    if options.debug_mode:
        print("Processed " + msg)
    else:
        print("Successfully transmitted " + msg)
    file_counter = byte_total = 0

def main(argv):
    version_check()
    get_options(argv)
    if options.trace >= 2 and options.mode.endswith("_end"):
        print_environment()
        print_options()

    validate_options()
    if options.trace and options.mode.endswith("_end"):
        print_options()

    if (options.mode == "refr_write_end" or options.mode == "integ_end") and options.filenames:
        file_loc_processing()
        report()
    if options.mode.endswith("_begin"):
        reset_processed_file()

if __name__ == "__main__":
    try:
        main(sys.argv)
        sys.stdout.flush() 
        sys.exit(0) 
    except Exception as err:
        sys.stdout.flush() 
        sys.stderr.write("F_JX0D01: {0}\n".format(err))
        report()
        sys.stderr.flush()
        sys.exit(1)

